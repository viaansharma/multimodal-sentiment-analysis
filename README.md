ğŸ§  Multi-Modal Sentiment Analysis (Text + Image)

An end-to-end Multi-Modal Sentiment Analysis system that predicts sentiment by jointly analyzing text and image data from social media posts.

The project uses Transformer-based text encoding (BERT) and CNN-based image feature extraction (ResNet-50), followed by a fusion-based classifier and a Streamlit web interface for interactive inference.

ğŸš€ Key Features

ğŸ“ Text Feature Extraction using BERT (Hugging Face Transformers)

ğŸ–¼ï¸ Image Feature Extraction using ResNet-50 (ImageNet pretrained)

ğŸ”— Late Fusion of text and image embeddings

âš–ï¸ Class-weighted loss to handle class imbalance

ğŸ”§ Selective fine-tuning of the last BERT layer

ğŸ“Š Strong evaluation performance (Macro-F1 = 0.74)

ğŸŒ Streamlit web app for real-time predictions

â˜ï¸ Works offline (full inference) and online (demo mode)

ğŸ“‚ Project Structure
multimodal-sentiment/
â”‚
â”œâ”€â”€ 01_dataset_loader.ipynb
â”œâ”€â”€ 02_text_preprocess.ipynb
â”œâ”€â”€ 03_image_preprocess.ipynb
â”œâ”€â”€ 04_multimodal_model.ipynb
â”œâ”€â”€ 05_train.ipynb
â”œâ”€â”€ 06_evaluate.ipynb
â”œâ”€â”€ 07_streamlit_app.ipynb
â”‚
â”œâ”€â”€ streamlit_app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ MVSA_Single/
â”‚   â”œâ”€â”€ data/                (not uploaded)
â”‚   â”œâ”€â”€ labelResultsAll.txt

ğŸ“Š Dataset

Dataset: MVSA-Single (public multimodal sentiment dataset)

Modalities: Text + Image

Classes: Negative, Neutral, Positive

Total Samples: 4,869

âš ï¸ Due to size and licensing constraints, the dataset images are not included in this repository.

ğŸ—ï¸ Model Architecture
ğŸ”¹ Text Encoder

Model: BERT-base-uncased

Output: 768-dimensional embedding

ğŸ”¹ Image Encoder

Model: ResNet-50 (pretrained on ImageNet)

Output: 2048-dimensional embedding

ğŸ”¹ Fusion & Classifier

Late fusion via concatenation

Fully connected layers with ReLU and Dropout

3-class sentiment classification

âš™ï¸ Training Strategy

Initial training with frozen encoders for stability

Class-weighted CrossEntropyLoss to address imbalance

Selective fine-tuning of the last BERT encoder layer

Optimizer: AdamW

This strategy significantly improved minority-class performance.

ğŸ“ˆ Results
Final Performance (Validation Set)
Metric	Score
Accuracy	0.74
Macro F1-Score	0.74
Confusion Matrix
[[186,  39,  19],
 [ 65, 274,  45],
 [ 36,  47, 263]]


The model achieves balanced performance across all sentiment classes.

ğŸŒ Streamlit Web App
â–¶ï¸ Run Locally (Full Inference)
streamlit run streamlit_app.py


Make sure multimodal_model.pth is present in the project root.

â˜ï¸ Online Deployment (Demo Mode)

The Streamlit app is designed to run safely online even when model weights are not included.

If multimodal_model.pth is missing:

The UI loads

A clear message explains how to run full inference locally

No runtime crash occurs

This follows best practices for ML deployment.

ğŸ§  Key Learnings

Multi-modal fusion improves sentiment understanding over unimodal approaches

Class imbalance must be explicitly handled

Partial fine-tuning offers strong gains with minimal overfitting

Notebook-based training and production deployment require careful separation

ğŸš€ Future Improvements

Attention-based fusion mechanisms

Multimodal Transformers (e.g., ViLBERT, CLIP)

Additional datasets (MVSA-Multiple)

Probability calibration for confidence estimation

ğŸ‘¨â€ğŸ’» Author

Viaan Sharma
M.Tech â€“ Mathematics & Computing (Machine Learning)
National Institute of Technology Delhi

