{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5fb93552-8d47-4955-b7a6-31029b3c5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cbadca0-9740-4103-8872-a3fb674ffce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4869, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_mvsa_dataset(base_path=\"MVSA_Single\"):\n",
    "    label_file = os.path.join(base_path, \"labelResultAll.txt\")\n",
    "    data_dir = os.path.join(base_path, \"data\")\n",
    "\n",
    "    records = []\n",
    "\n",
    "    with open(label_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "\n",
    "        img_id = parts[0]\n",
    "        sentiments = parts[1].split(\",\")\n",
    "\n",
    "        if len(sentiments) != 2:\n",
    "            continue\n",
    "\n",
    "        text_sentiment = sentiments[0].strip().lower()\n",
    "        label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "        if text_sentiment not in label_map:\n",
    "            continue\n",
    "\n",
    "        text_path = os.path.join(data_dir, f\"{img_id}.txt\")\n",
    "        image_path = os.path.join(data_dir, f\"{img_id}.jpg\")\n",
    "\n",
    "        if not os.path.exists(text_path) or not os.path.exists(image_path):\n",
    "            continue\n",
    "\n",
    "        with open(text_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as t:\n",
    "            text = t.read().strip()\n",
    "\n",
    "        records.append({\n",
    "            \"text\": text,\n",
    "            \"image_path\": image_path,\n",
    "            \"label\": label_map[text_sentiment]\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df = load_mvsa_dataset()\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88555efc-98f5-4850-b5a2-5382f8b403af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3895, 3) (974, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(train_df.shape, val_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5b830b7-6cb7-4f5a-a647-c3e736478e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "class MVSADataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            row[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        image = image_transform(image)\n",
    "\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "\n",
    "        return (\n",
    "            encoding[\"input_ids\"].squeeze(0),\n",
    "            encoding[\"attention_mask\"].squeeze(0),\n",
    "            image,\n",
    "            label\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c26c6244-c277-477b-a177-2354c109302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    MVSADataset(train_df),\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    MVSADataset(val_df),\n",
    "    batch_size=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62a195fb-90c8-45a8-bae5-992623e7a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from transformers import BertModel\n",
    "\n",
    "class MultiModalSentimentModel(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Text encoder\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # Image encoder\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "\n",
    "        # Fusion + classifier\n",
    "        self.fc1 = nn.Linear(768 + 2048, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        text_features = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).pooler_output\n",
    "\n",
    "        image_features = self.resnet(images)\n",
    "\n",
    "        fused = torch.cat((text_features, image_features), dim=1)\n",
    "        x = self.fc1(fused)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88c8dbe9-6360-44e2-ac19-ff5d795ef495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultiModalSentimentModel().to(device)\n",
    "\n",
    "# Class-weighted loss\n",
    "class_counts = df[\"label\"].value_counts().sort_index()\n",
    "weights = 1.0 / class_counts\n",
    "weights = torch.tensor(weights.values, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     filter(lambda p: p.requires_grad, model.parameters()),\n",
    "#     lr=1e-3\n",
    "# )\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=2e-5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a71e58e5-7b9d-4298-99e3-333a3b7db7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze BERT\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Unfreeze ONLY the last BERT encoder layer\n",
    "for param in model.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Freeze ResNet\n",
    "for param in model.resnet.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e406afb-b5ba-48dc-aaa3-841822223302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.8516178910737165\n",
      "Epoch 2 Loss: 0.5651495594538947\n"
     ]
    }
   ],
   "source": [
    "# epochs = 5\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for input_ids, attention_mask, images, labels in train_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss:\", total_loss / len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66df2872-c16b-42aa-be4e-5b1391f368af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as multimodal_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"multimodal_model.pth\")\n",
    "\n",
    "print(\"Model saved as multimodal_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b6483f-77ff-4391-92c1-61fa7129367d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
